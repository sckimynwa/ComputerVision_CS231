# ComputerVision_CS231
Stanford ComputerVision Study

## Syllabus

Lecture 1
> Computer vision overview
> Historical Context

Lecture 2
> Image Classification
> Data-driven Approach
> K-nearest neighbor
> Linear classification

Lecture 3
> Higher-Level representations
> Optimization, stochastic gradient descent

Lecture 4
> Backpropagation
> Multi-Layer Perceptrons
> Neuron Viewpoint

Lecture 5
> Convolutional Neural Network (CNN)
> http://taewan.kim/post/cnn/

Lecture 6
> Traning Neural Network
> Sigmoid Function Cons
> - Gradient Vanishing
> - Gradient always positive
> various activation function
> Proper Initialization
> Batch Normalization
> Babysitting the Learning Process

Lecture 7
> Activation funtion revisited
> Gradient Descent
    - SGD (Stochastic Gradient Descent)
    - SGD + Momentum
    - SGD + Nesterov Momentum
    - AdaGrad
    - RmsProp
    - Adam

## Assignments

Assignment 1
> features
> knn (kth nearest neighbor)
> softmax function
> svm
> two_layer_network

About svm loss gradient explanation
- https://mlxai.github.io/2017/01/06/vectorized-implementation-of-svm-loss-and-gradient-update.html